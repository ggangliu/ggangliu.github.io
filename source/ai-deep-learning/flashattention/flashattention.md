# Flash Attention

FlashAttention是一种IO-觉察的精确注意算法，用平铺（tiling）来减少GPU高带宽存储器（HBM）和GPU片上SRAM之间的存储器读/写次数。

![flash](./flashattn_banner.jpg)

![benchmark](flash2_a100_fwd_bwd_benchmark.png)
