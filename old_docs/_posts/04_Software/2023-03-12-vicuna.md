---
layout: post
tile: "Vicuna(小羊驼)"
tags: other-chips
---

## 背景

UC伯克利联手CMU、斯坦福、UCSD和MBZUAI，推出了130亿参数的Vicuna，俗称小羊驼，能够实现ChatGPT 90%的性能。

Vicuna是通过使用从ShareGPT收集到的大约7万个用户共享的对话与公共API来微调一个LLaMA基础模型而创建的

Vicuna的权重只需单个GPU就能跑

项目地址 <https://github.com/lm-sys/FastChat>

## 130亿参数，90%匹敌ChatGPT

Vicuna是通过在ShareGPT收集的用户共享对话上对LLaMA进行微调训练而来，训练成本近300美元。

测试过程使用GPT-4作为评判标准，结果显示Vicuna-13B在超过90%的情况下实现了与ChatGPT和Bard相匹敌的能力

## 训练

Vicuna-13B的训练流程如下：

首先，研究人员从ChatGPT对话分享网站ShareGPT上，收集了大约70K对话。接下来，研究人员优化了Alpaca提供的训练脚本，使模型能够更好地处理多轮对话和长序列。之后利用PyTorch FSDP在8个A100 GPU上进行了一天的训练。

## 安装

```bat
git clone https://github.com/lm-sys/FastChat.git
cd FastChat
pip3 install --upgrade pip  # enable PEP 660 support
pip3 install -e .
```

### 权重

根据LLaMA模型的许可，权重将以delta的形式发布。只需将其加到原来的LLaMA权重上，就可以获得最终的Vicuna权重。

1. 按照huggingface上的说明，获得原始的LLaMA权重
2. 通过脚本，自动从团队的Hugging Face账户上下载delta权重

    ```bat
    python3 -m fastchat.model.apply_delta \
        --base /path/to/llama-13b \
        --target /output/path/to/vicuna-13b \
        --delta lmsys/vicuna-13b-delta-v0
    ```

### 使用

- 单个GPU

  Vicuna-13B需要大约28GB的GPU显存

  ```bat
  python3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights
  ```

- 仅CPU

  如果想在CPU上运行，则需要大约60GB的内存

  ```bat
  python3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights --device cpu
  ```

- Web UI
  - 启动控制器
    > python3 -m fastchat.serve.controller
  - 启动model worker
    > python3 -m fastchat.serve.model_worker --model-path /path/to/vicuna/weights
  - 发送测试消息
    > python3 -m fastchat.serve.test_message --model-name vicuna-13b
  - 启动gradio网络服务器
    > python3 -m fastchat.serve.gradio_web_server
  
  现在，你就可以打开浏览器和模型聊天了。

## Reference

- <https://vicuna.lmsys.org/>
- <https://zhuanlan.zhihu.com/p/619403126>
- <https://lmsys.org/blog/2023-03-30-vicuna>
