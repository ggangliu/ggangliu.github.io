<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformer &mdash; ggangliu-doc v0.01 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/_variables.scss?v=7050c318" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=f6c7d6a8"></script>
        <script src="../../_static/doctools.js?v=888ff710"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="vLLM" href="../vllm/vllm.html" />
    <link rel="prev" title="Artificial Intelligence for Beginners" href="../ai-for-beginners.html" />
    <link href="../../_static/custom.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            ggangliu-doc
          </a>
              <div class="version">
                v0.01
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../ai-deep-learning.html">AI (Deep Learning)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../deep-learning-course.html">How to study Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning_pytorch.html">Learning Pytorch with Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rnn/rnn.html">Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../diffusion/diffusion.html">Diffusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sd3/stable-diffusion-3-medium.html">Stable Diffusion 3 Medium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative-ai-for-beginners.html">Generative AI for Beginners (Version 2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai-for-beginners.html">Artificial Intelligence for Beginners</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Transformer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#rnn">RNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cnn">CNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transformer-architecture">Transformer Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#encoder">Encoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#attention">Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-head-attention">Multi-head attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="#self-attention">Self-Attention</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#practices">Practices</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mamba">Mamba</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../vllm/vllm.html">vLLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lightllm/lightllm.html">LightLLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tokenattention/tokenattention.html">Token Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../flashattention/flashattention.html">Flash Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faster-transformer/faster-transformer.html">FasterTransformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kimi/mooncake.html">Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-embedded.html">AI Embedded</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simulator.html">Simulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computer-architecture.html">Computer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hdl.html">Hardware Description Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../open-source-project.html">Open Source Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../work-related.html">Work-related</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ggangliu-doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../ai-deep-learning.html">AI (Deep Learning)</a></li>
      <li class="breadcrumb-item active">Transformer</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="transformer">
<h1>Transformer<a class="headerlink" href="#transformer" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a>
<a class="reference external" href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></p>
<p>Transformers is the new simple yet powerful neural network architecture introduced by Google Brain in 2017 with their famous research paper “Attention is all you need.” It is based on the attention mechanism instead of the sequential computation as we might observe in recurrent networks.</p>
<section id="rnn">
<h2>RNN<a class="headerlink" href="#rnn" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Suffers issues with long-range dependencies. RNNs do not work well with long text documents.</p></li>
<li><p>Suffers from gradient vanishing and gradient explosion.</p></li>
<li><p>RNNs need larger training steps to reach a local/global minima.</p></li>
<li><p>RNNs do not allow parallel computation.</p></li>
</ul>
</section>
<section id="cnn">
<h2>CNN<a class="headerlink" href="#cnn" title="Link to this heading"></a></h2>
</section>
<section id="transformer-architecture">
<h2>Transformer Architecture<a class="headerlink" href="#transformer-architecture" title="Link to this heading"></a></h2>
<section id="encoder">
<h3>Encoder<a class="headerlink" href="#encoder" title="Link to this heading"></a></h3>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>Stage1_out = Embedding512 + TokenPositionEncoding512
Stage2_out = layer_normalization(multihead_attention(Stage1_out) + Stage1_out)
Stage3_out = layer_normalization(FFN(Stage2_out) + Stage2_out)

out_enc = Stage3_out
</pre></div>
</div>
</section>
<section id="decoder">
<h3>Decoder<a class="headerlink" href="#decoder" title="Link to this heading"></a></h3>
<div class="highlight-code notranslate"><div class="highlight"><pre><span></span>Stage1_out = OutputEmbedding512 + TokenPositionEncoding512

Stage2_Mask = masked_multihead_attention(Stage1_out)
Stage2_Norm1 = layer_normalization(Stage2_Mask) + Stage1_out
Stage2_Multi = multihead_attention(Stage2_Norm1 + out_enc) +  Stage2_Norm1
Stage2_Norm2 = layer_normalization(Stage2_Multi) + Stage2_Multi

Stage3_FNN = FNN(Stage2_Norm2)
Stage3_Norm = layer_normalization(Stage3_FNN) + Stage2_Norm2

out_dec = Stage3_Norm
</pre></div>
</div>
</section>
</section>
<section id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Link to this heading"></a></h2>
<p>Attention between encoder and decoder is crucial in NMT. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<section id="scaled-dot-product-attention">
<h3>Scaled Dot-Product Attention<a class="headerlink" href="#scaled-dot-product-attention" title="Link to this heading"></a></h3>
<p>In terms of encoder-decoder, the query is usually the hidden state of the decoder. Whereas key, is the hidden state of the encoder, and the corresponding value is normalized weight, representing how much attention a key gets. Output is calculated as a wighted sum – here the dot product of query and key is used to get a value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">denum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">num</span> <span class="o">/</span> <span class="n">denum</span><span class="p">),</span> <span class="n">V</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="multi-head-attention">
<h3>Multi-head attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading"></a></h3>
<p>Transformer reduces the number of operations required to relate (especially distant) positions in input and output sequence to a <span class="math notranslate nohighlight">\(O(1)\)</span>.</p>
<p>Transformer use multi-head (<span class="math notranslate nohighlight">\(d_{model}/h\)</span> parallel attention functions) attention instead of single (<span class="math notranslate nohighlight">\(d_{model}\)</span>-dimensional) attention function.</p>
</section>
<section id="self-attention">
<h3>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading"></a></h3>
<p>In encoder, self-attention layers process input <span class="math notranslate nohighlight">\(queries, keys\)</span> and <span class="math notranslate nohighlight">\(values\)</span> that comes form same place i.e. the output of previous layer in encoder. Each position in encoder can attend to all positions from previous layer of the encoder</p>
<p>In decoder, self-attention layer enable each position to attend to all previous positions in the decoder, including the current position. To preserve auto-regressive property, the leftward information flow is presented inside the dot-product attention by masking out (set to <span class="math notranslate nohighlight">\(- \infty\)</span>) all <span class="math notranslate nohighlight">\(values\)</span> that are input for softmax which correspond to this illegal connections.</p>
<p><img alt="Tranformer step-by-step sequence" src="../../_images/transform20fps.gif" /></p>
<p>In encoder phase (shown in the Figure 1.), transformer first generates initial representation/embedding for each word in input sentence (empty circle). Next, for each word, self-attention aggregates information form all other words in context of sentence, and creates new representation (filled circles). The process is repeated for each word in sentence. Successively building new representations, based on previous ones is repeated multiple times and in parallel for each word (next layers of filled circles).</p>
<p>Decoder acts similarly generating one word at a time in a left-to-right-pattern. It attends to previously generated words of decoder and final representation of encoder.</p>
</section>
</section>
<section id="practices">
<h2>Practices<a class="headerlink" href="#practices" title="Link to this heading"></a></h2>
<p>What are the main components of transformers?</p>
<p><img alt="Alt text" src="../../_images/image-141.png" /></p>
<p><img alt="Alt text" src="../../_images/image-212.png" /></p>
<ol class="arabic">
<li><p>Input Embedding
Token 将文本转换为数字，每个token一个向量(1, 512)
Inputs = “我爱你”
“我” = [1, 0, 0, …, 0](1, 10000)
“爱” = [0, 1, 0, …, 0](1, 10000)
“你” = [0, 0, 1, …, 0](1, 10000)
Embedd Space = (10000, 512)
<img alt="Alt text" src="../../_images/image-82.png" /></p></li>
<li><p>Positional Encoding
给文字加位置信息(不同语境下的语义信息)</p>
<ol class="arabic simple">
<li><p>每个单词的token都能包含它的位置信息，奇数sin偶数cos</p></li>
<li><p>模型可以看到文字之间的”距离”</p></li>
<li><p>模型可以看懂并学习到位置编码的规则</p></li>
<li><p><img alt="Alt text" src="../../_images/image-92.png" /></p></li>
</ol>
</li>
<li><p>Multi-Head Attention
不同类型的相关度，语义关系学习
“Bark is very cute and he is a dog”.
Here, if we take the word ‘dog’, the dog’s name is Bark, it is a male dog, and that he is a cute dog.
<img alt="Alt text" src="../../_images/image-221.png" /></p>
<ol class="arabic simple">
<li><p>Linear 将nx512拆分为multi-head nx8x64 =&gt; 8xnx64</p></li>
</ol>
<p><img alt="Alt text" src="../../_images/image-191.png" />
X = [batch_size, ctx_length, d_model]
Wq = [d_model, d_model] =&gt; Q = X &#64; Wq = [batch_size, ctx_length, d_model]
Wk = [d_model, d_model] =&gt; K = X &#64; Wk =
Wv = [d_model, d_model] =&gt; V = X &#64; Wv =
<img alt="Alt text" src="../../_images/image-101.png" /></p>
<p><strong>切分multi-head Self-attention</strong></p>
<p>Different types of relevance 不同类型的相关度</p>
<p><img alt="Alt text" src="../../_images/image-201.png" />
[batch_size, ctx_length, num_heads, d_model/num_heads] =&gt; [batch_size, num_heads, ctx_length, d_model/num_heads]</p>
<p>Q &#64; Kt = [batch_size, num_head, ctx_length, d_model/num_heads] &#64; [batch_size, num_head, d_model/num_heads, ctx_length]</p>
</li>
<li><p>Add &amp; Norm</p></li>
<li><p>Feed Forward 全连接的前馈网络</p></li>
<li><p>Softmax
Q &#64; K = [ctx_length, ctx_length]</p>
<p><strong>Mask</strong></p>
<p>QK [4, 4, 16, 16] &#64; V [batch_siz, num_head, ctx_length, 128] = A [4, 4, 16, 128]</p>
<p><strong>Output</strong>
Wo [d_model, d_model]</p>
<p>A &#64; Wo = Output [batch_size, ctx_length, d_model]</p>
</li>
</ol>
<p><img alt="Alt text" src="../../_images/image-114.png" /></p>
<p>BERT (Bidirectional Encoder Representations from Transformers)
GPT (General Purpose Transformer)</p>
</section>
<section id="mamba">
<h2>Mamba<a class="headerlink" href="#mamba" title="Link to this heading"></a></h2>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a></p></li>
<li><p><a class="reference external" href="https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face">https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face</a></p></li>
<li><p><a class="reference external" href="https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch">https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch</a></p></li>
<li><p><a class="reference external" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php">https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb">https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb</a></p></li>
<li><p><a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p></li>
<li><p><a class="reference external" href="https://github.com/karpathy/ng-video-lecture">https://github.com/karpathy/ng-video-lecture</a></p></li>
</ul>
<p>Example:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/P3n9W31/transformer-pytorch">https://github.com/P3n9W31/transformer-pytorch</a></p></li>
</ul>
<p><img alt="Alt text" src="../../_images/image-52.png" />
<img alt="Alt text" src="../../_images/image-72.png" />
<img alt="Alt text" src="../../_images/transformer_decoding_1.gif" />
<img alt="Alt text" src="../../_images/transformer_decoding_2.gif" />
<img alt="Alt text" src="../../_images/image-131.png" /></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../ai-for-beginners.html" class="btn btn-neutral float-left" title="Artificial Intelligence for Beginners" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../vllm/vllm.html" class="btn btn-neutral float-right" title="vLLM" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ggangliu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>