

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Jamba &mdash; ggangliu-doc v0.01 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/_variables.scss?v=a5c9e1b9" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0f1f8bc0" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f6c7d6a8"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link href="../../_static/custom.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            ggangliu-doc
              <img src="../../_static/firstai-2.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../ai-deep-learning.html">AI (Deep Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-embedded.html">AI Embedded</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simulator.html">Simulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computer-architecture.html">Computer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hdl.html">Hardware Description Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../open-source-project.html">Open Source Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../work-related.html">Work-related</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ggangliu-doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Jamba</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="jamba">
<h1>Jamba<a class="headerlink" href="#jamba" title="Link to this heading"></a></h1>
<p>Jamba is a state-of-the-art, hybrid SSM-Transformer LLM. Jamba is the first production-scale Mamba implementation, which opens up interesting research and application opportunities.</p>
<p>This model card is for the base version of Jamba. It’s a pretrained, mixture-of-experts (MoE) generative text model, with 12B active parameters and a total of 52B parameters across all experts. It supports a 256K context length, and can fit up to 140K tokens on a single 80GB GPU.</p>
<section id="paper">
<h2>Paper<a class="headerlink" href="#paper" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/2403.19887.pdf">Jamba:A Hybrid Transformer-Mamba Language Model</a></p>
<p>Model: <a class="reference external" href="https://huggingface.co/ai21labs/Jamba-v0.1">https://huggingface.co/ai21labs/Jamba-v0.1</a></p>
<ul class="simple">
<li><p>we end up with a powerful model that fits in a single 80GB GPU</p></li>
<li><p>the model presents strong results for up to 256K tokens context length</p></li>
<li><p>Taking advantage of both model families, Jamba combines Transformer and Mamba layers, at a certain ratio. Varying the ratio of Transformer/Mamba layers allows balancing memory usage, efficient training, and long context capabilities.</p></li>
</ul>
<section id="model-architecture">
<h3>Model Architecture<a class="headerlink" href="#model-architecture" title="Link to this heading"></a></h3>
<p>Jamba is a hybrid decoder architecture
<img alt="Alt text" src="../../_images/image1.png" /></p>
<ol class="arabic simple">
<li><p>the KV cache – the memory required to store the attention keys and values in the context. When scaling Transformer models to long contexts, the KV cache becomes a limiting factor. Trading off attention layers for Mamba layers reduces the total size of the KV cache.</p></li>
</ol>
<p>Our architecture aims to provide not only a small number of active parameters but also an 8x smaller KV cache compared to a vanilla Transformer.</p>
<p><img alt="Alt text" src="../../_images/image-11.png" /></p>
</section>
<section id="reaping-the-benefits">
<h3>Reaping the Benefits<a class="headerlink" href="#reaping-the-benefits" title="Link to this heading"></a></h3>
<section id="jamba-implementation-for-a-single-80gb-gpu">
<h4>Jamba Implementation for a Single 80GB GPU<a class="headerlink" href="#jamba-implementation-for-a-single-80gb-gpu" title="Link to this heading"></a></h4>
<p>Jamba Implementation for a Single 80GB GPU. In our implementation we have a sequence of 4 Jamba blocks. Each Jamba block has the following configuration:</p>
<ul class="simple">
<li><p>l = 8: The number of layers.</p></li>
<li><p>a : m = 1 : 7: ratio attention-to-Mamba layers.</p></li>
<li><p>e = 2: how often to use MoE instead of a single MLP.</p></li>
<li><p>n = 16: total number of experts.</p></li>
<li><p>K = 2: number of top experts used at each token</p></li>
</ul>
<p><img alt="Alt text" src="../../_images/image-21.png" /></p>
<p>Overall, our Jamba implementation was successfully trained on context lengths of up to 1M tokens. The released model supports lengths of up to 256K tokens.</p>
</section>
<section id="throughput-analysis">
<h4>Throughput Analysis<a class="headerlink" href="#throughput-analysis" title="Link to this heading"></a></h4>
<p><img alt="Alt text" src="../../_images/image-31.png" /></p>
</section>
</section>
<section id="training-infrastructure-and-dataset">
<h3>Training Infrastructure and Dataset<a class="headerlink" href="#training-infrastructure-and-dataset" title="Link to this heading"></a></h3>
<p>The model was trained on NVIDIA H100 GPUs.</p>
</section>
<section id="evaluation">
<h3>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading"></a></h3>
<section id="academic-benchmarks">
<h4>Academic Benchmarks<a class="headerlink" href="#academic-benchmarks" title="Link to this heading"></a></h4>
<p><img alt="Alt text" src="../../_images/image-4.png" /></p>
<p>In summary, Jamba demostrates the ability of hybrid architectures to reach the performance of state-of-the-art Transformer based models of the same size class, while having the benefits of anSSM.</p>
</section>
<section id="long-context-evaluations">
<h4>Long-Context Evaluations<a class="headerlink" href="#long-context-evaluations" title="Link to this heading"></a></h4>
<p>We have successfully trained Jamba models with context lengths of up to 1M tokens.</p>
</section>
</section>
<section id="ablations-and-insights">
<h3>Ablations and Insights<a class="headerlink" href="#ablations-and-insights" title="Link to this heading"></a></h3>
<p>we found useful: explicit positional information is not needed in Jamba, and
Mamba layers necessitate special normalization to stabilize training at large scale.</p>
<p><img alt="Alt text" src="../../_images/image-5.png" /></p>
<p><img alt="Alt text" src="../../_images/image-6.png" /></p>
<section id="the-effect-of-mixture-of-experts-moe">
<h4>The Effect of Mixture-of-Experts (MoE)<a class="headerlink" href="#the-effect-of-mixture-of-experts-moe" title="Link to this heading"></a></h4>
<p><img alt="Alt text" src="../../_images/image-7.png" /></p>
</section>
<section id="stabilizing-mamba-at-large-scale">
<h4>Stabilizing Mamba at large scale<a class="headerlink" href="#stabilizing-mamba-at-large-scale" title="Link to this heading"></a></h4>
<p><img alt="Alt text" src="../../_images/image-8.png" /></p>
</section>
<section id="jamba-does-not-require-explicit-positional-information">
<h4>Jamba does not Require Explicit Positional Information<a class="headerlink" href="#jamba-does-not-require-explicit-positional-information" title="Link to this heading"></a></h4>
<p><img alt="Alt text" src="../../_images/image-9.png" /></p>
</section>
</section>
</section>
<section id="intallation">
<h2>Intallation<a class="headerlink" href="#intallation" title="Link to this heading"></a></h2>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/huggingface/transformers
</pre></div>
</div>
<p>Install from local source</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/huggingface/transformers.git
<span class="nb">cd</span><span class="w"> </span>transformers
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
<p>Jamba requires you use transformers version 4.39.0 or higher:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mamba-ssm<span class="w"> </span>causal-conv1d&gt;<span class="o">=</span><span class="m">1</span>.2.0
pip<span class="w"> </span>install<span class="w"> </span>transformers&gt;<span class="o">=</span><span class="m">4</span>.39.0
</pre></div>
</div>
<p><a class="reference external" href="https://huggingface.co/ai21labs/Jamba-v0.1">https://huggingface.co/ai21labs/Jamba-v0.1</a></p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ggangliu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>