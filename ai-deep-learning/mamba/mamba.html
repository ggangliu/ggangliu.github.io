

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Understanding Mamba &mdash; ggangliu-doc v0.01 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/_variables.scss?v=a5c9e1b9" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0f1f8bc0" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f6c7d6a8"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link href="../../_static/custom.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            ggangliu-doc
              <img src="../../_static/firstai-2.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../ai-deep-learning.html">AI (Deep Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-embedded.html">AI Embedded</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simulator.html">Simulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computer-architecture.html">Computer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hdl.html">Hardware Description Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../open-source-project.html">Open Source Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../work-related.html">Work-related</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ggangliu-doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Understanding Mamba</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="understanding-mamba">
<h1>Understanding Mamba<a class="headerlink" href="#understanding-mamba" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a>
<a class="reference external" href="https://arxiv.org/abs/2312.00752">https://arxiv.org/abs/2312.00752</a></p>
<section id="paper">
<h2>Paper<a class="headerlink" href="#paper" title="Link to this heading"></a></h2>
<p>structured state space models (SSMs) have been developed to address Transformers’ computational inefficiency on long sequences.</p>
<p>We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.</p>
<p><strong>Selection Mechanism</strong> allows the model to filter out irrelevant information and remember relevant information indefinitely</p>
<p><strong>Hardware-aware Algorithm</strong> computes the model recurrently with a scan instead of convolution, scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs</p>
<p><strong>Architecture</strong></p>
<p>Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. S4 models are defined with four parameters (∆, A, B, C), which define a sequence-to-sequence transformation in two stag.</p>
</section>
<section id="rnn">
<h2>1. RNN<a class="headerlink" href="#rnn" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>训练慢 空间复杂度O(n)</p></li>
<li><p>推理快 时间复杂度O(1)</p></li>
</ul>
<p>传统RNN的典型公式(有一个非线性的tanh())
<img alt="Alt text" src="../../_images/image-91.png" /></p>
<p><img alt="Alt text" src="../../_images/image-51.png" /></p>
</section>
<section id="transformer">
<h2>2. Transformer<a class="headerlink" href="#transformer" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>训练快(训练过程可以并行化计算)</p></li>
<li><p>推理慢
<img alt="Alt text" src="../../_images/image-41.png" /></p></li>
</ul>
</section>
<section id="ssm">
<h2>3. SSM<a class="headerlink" href="#ssm" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>训练快(并行训练)</p></li>
<li><p>推理快(递归预测，是Transformer的5倍)</p></li>
</ul>
<p>Mamba 最独特的地方在于它脱离了传统的注意力和 MLP（多层感知器）模块。这种简化使得模型更轻便、更快速，并且能随着序列长度线性地缩放，这是之前模型无法做到的。</p>
<section id="id1">
<h3>基础模型<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>连续状态的时不变系统
<img alt="Alt text" src="../../_images/image-12.png" /></p>
<p>该系统的状态方程可以表示为(A, B, C, D均为可以学习的矩阵)：
<img alt="Alt text" src="../../_images/image-13.png" /></p>
<p>其中D被称为skip connection, 可以理解为经过变换的残差连接。因此通常把不包括在内的部分特指为SSM。</p>
<p><img alt="Alt text" src="../../_images/image-24.png" />
该系统有两个点需要强调：</p>
<ol class="arabic simple">
<li><p>该系统为一个时不变系统（对后面mamba的改造非常关键）；</p></li>
<li><p>该系统是一个连续系统（实际处理中需要离散化）；</p></li>
</ol>
</section>
<section id="id2">
<h3>离散化及其解法<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>微分极限法（微分定义）</p></li>
<li><p>双线性变换（积分思想）</p></li>
<li><p>零序保持（Mamba）</p></li>
</ol>
<p><img alt="Alt text" src="../../_images/image-16.png" /></p>
<p>离散化之后状态方程就变为如下，经过离散化后就实现了线性的循环模式
<img alt="Alt text" src="../../_images/image-17.png" /></p>
<p>由此可以写出前几步的状态变量，即状态X可以表示成输入的函数。
<img alt="Alt text" src="../../_images/image-18.png" /></p>
<p>同理可以写出前几步的输出变量
<img alt="Alt text" src="../../_images/image-19.png" />
卷积核
<img alt="Alt text" src="../../_images/image-20.png" />
卷积计算过程
<img alt="Alt text" src="../../_images/image-211.png" />
为了计算K，关键在于计算A，最理想的状态是特征对角化。除此之外还有一种HiPPO(High-Order Polynomial Projection Operator) 矩阵的初始化方法，即
<img alt="Alt text" src="../../_images/image-22.png" /></p>
<p>在Transformer架构中，前文的信息存储在了KV Cache中，而SSM中不存在类似模块，因此其循环形式并不擅长处理上下文相关的学习问题，那么就引出了Mamba的优化和改进。</p>
</section>
</section>
<section id="mamba">
<h2>Mamba的突围<a class="headerlink" href="#mamba" title="Link to this heading"></a></h2>
<p>概况而言，Mamba主要解决的问题有以下两方面：</p>
<ol class="arabic simple">
<li><p>选择性复制：在语言模型的上下文中，选择性复制是指从给定输入中辨别相关信息并将其适当地再显或合并到生成的输出中的能力。涉及模型从输入数据中识别和重现特定短语、实体或模式的能力，从而增强生成文本的相关性和连贯性</p></li>
<li><p>归纳头：语言模型中的归纳头输入专门组件，可促进模型从输入数据中推断和概括知识的能力。与人类根据观察到的模式得出结论和进行推论的方式类似，归纳头使模型能够推断信息、理解潜在的关系，并应用学到的概念来生成更细致、更适合上下文的响应。</p></li>
</ol>
<section id="id3">
<h3>Mamba的选择机制<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<p>普通SSM无法学习选择性复制任务的原因是它们是时不变的。这意味这可学习参数随着每个新token的传入而保持固定。</p>
<p>Mamba的第一个改进就是选择性扫描(Selective Scan)，即放弃卷积核递归的双重属性，仅依赖于循环。具体来说就是参数时变化。矩阵A保持不变，但B和C现在成为了输入的函数，也就是系统由时不变成为了时变系统。通过下面的算法可以理解两者的差异。
<img alt="Alt text" src="../../_images/image-32.png" /></p>
</section>
<section id="id4">
<h3>训练加速–并行扫描<a class="headerlink" href="#id4" title="Link to this heading"></a></h3>
<p>由于上述操作仅具有循环形式，不具有卷积形式，那么训练就无法实现加速并行化。</p>
<p>下图算法为并行扫描，为其工作原理：
<img alt="Alt text" src="../../_images/image-10.png" /></p>
<p>Mamba不可并行化(因为它是时变的)，因此需要依赖循环操作。Mamba的作者采用三种经典技术来提供循环操作速度：</p>
<ul class="simple">
<li><p>并行扫描算法(Parallel Scan)</p></li>
<li><p>核融合(Kernel Fusion)</p></li>
<li><p>激活重计算(Activation Recomputation)</p></li>
</ul>
</section>
<section id="id5">
<h3>Mamba的结构与实现<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<p>Mamba模型是由多层Mamba层连接而成，与Transformer的模型非常相似。Mamba层是H3和门控MLP操作的组合。
<img alt="Alt text" src="../../_images/image-25.png" />
<img alt="Alt text" src="../../_images/image-231.png" /></p>
<p><img alt="Alt text" src="../../_images/image-14.png" />
<img alt="Alt text" src="../../_images/image-15.png" /></p>
<p><strong>After the parameters have been transformed from (∆, A, B, C) ↦ (A, B, C), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3). Commonly, the model uses the convolutional mode (3) for efficient parallelizable training where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where
the inputs are seen one timestep at a time).</strong></p>
<p>是一个混合了CNN与RNN的模型，既有CNN的并行优势，又有RNN的长程感知能力。这里 ht-1 与 ht 是一个典型的 RNN 型计算，而 Bt，A，Ct，dt 这些计算又都是一些类似 y=wx+b 这样的类似 CNN 的运行或者说标准的神经网络计算。所以它即有良好的并行能力，又有 RNN 的记忆优势。</p>
<p><img alt="Alt text" src="../../_images/image-110.png" /></p>
<p><img alt="Alt text" src="../../_images/image-71.png" /></p>
<p>这里的K是卷积核. 这个卷积核非常大, 大到长度等于输入sequence length. 因为这个 kernel 的长度和 sequence length 一样, 所以原文又称之为 global attention.</p>
<p><img alt="Alt text" src="../../_images/image-81.png" /></p>
<p>更 fundamental 的理解是:</p>
<ol class="arabic simple">
<li><p>不同的yt是可以独立计算的(但独立计算是有代价的，本来yt可以通过递归的形式从yt-1一步算出来，但为了独立计算yt，要从头开始计算每个yt)</p></li>
<li><p>每个yb计算的pattern类似(共用一个 kernel 去和输入做 dot product)</p></li>
</ol>
<p>但独立计算似乎不是根本追求，根本追求是能不能并行计算，只有并行计算才能高效的利用GPU。</p>
<p><img alt="Alt text" src="../../_images/image-111.png" /></p>
</section>
<section id="ssm-state-space-models">
<h3>3.1 SSM(State Space Models)<a class="headerlink" href="#ssm-state-space-models" title="Link to this heading"></a></h3>
<p>Structured state space sequence models (S4)</p>
</section>
<section id="selective-state-space-models">
<h3>3.2 Selective State Space Models<a class="headerlink" href="#selective-state-space-models" title="Link to this heading"></a></h3>
<p>选择性SSM：通过选择性 SSM，Mamba 能够过滤掉不相关信息，专注于重要数据，从而更高效地处理序列。</p>
<p>S4 与 S6 的唯一计算区别就是参数 B 、 C 、 d∆ 的区别
<img alt="Alt text" src="../../_images/image-32.png" />
time-invariant -&gt; time-variant
从时不变到时变</p>
<section id="hardware-aware">
<h4>3.2.1 Hardware-aware<a class="headerlink" href="#hardware-aware" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Memory
将所有的SSM参数(A,B,C)加载到SRAM中，再执行discreization和recurrence in SRAM，再将最终结果写回DRAM</p></li>
<li><p>Kernel fusion
hidden特征先写会DRAM，每次计算再读入，改进为多次计算融合，单词计算不再写回DRAM，多次计算后将结果保存回DRAM</p></li>
</ul>
</section>
</section>
</section>
<section id="diffusion-based-with-structured-state-space-model">
<h2>4. Diffusion-based with Structured State Space Model<a class="headerlink" href="#diffusion-based-with-structured-state-space-model" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>用Transformer替换UNet</p></li>
<li><p>用SSM替换UNet</p></li>
<li><p>用Mamba替换UNet</p></li>
</ul>
</section>
<section id="reference">
<h2>5. Reference<a class="headerlink" href="#reference" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/state-spaces/mamba">https://github.com/state-spaces/mamba</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/680833040">https://zhuanlan.zhihu.com/p/680833040</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ggangliu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>