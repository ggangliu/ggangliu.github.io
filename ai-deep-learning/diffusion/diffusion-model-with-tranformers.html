

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Diffusion Transformer (DiT) Models &mdash; ggangliu-doc v0.01 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/_variables.scss?v=a5c9e1b9" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0f1f8bc0" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f6c7d6a8"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link href="../../_static/custom.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            ggangliu-doc
              <img src="../../_static/firstai-2.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../ai-deep-learning.html">AI (Deep Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-embedded.html">AI Embedded</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simulator.html">Simulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computer-architecture.html">Computer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hdl.html">Hardware Description Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../open-source-project.html">Open Source Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../work-related.html">Work-related</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ggangliu-doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Diffusion Transformer (DiT) Models</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="diffusion-transformer-dit-models">
<h1>Diffusion Transformer (DiT) Models<a class="headerlink" href="#diffusion-transformer-dit-models" title="Link to this heading"></a></h1>
<p>What is a Diffusion Transformer (DiT)? Diffusion Transformer (DiT) is a class of diffusion models that are based on the transformer architecture.</p>
<section id="introduction-to-diffusion-models">
<h2>Introduction to Diffusion Models<a class="headerlink" href="#introduction-to-diffusion-models" title="Link to this heading"></a></h2>
<p>Diffusion models are a type of generative model that simulates a Markov chain to transition from a simple prior distribution to the data distribution. The process is akin to a particle undergoing Brownian motion, where each step is a small random walk. This is why they are called “diffusion” models.</p>
<p>One of the key advantages of diffusion models is their ability to generate high-quality samples, which makes them particularly useful in tasks such as image synthesis.</p>
</section>
<section id="convolutional-u-net-architecture">
<h2>Convolutional U-NET Architecture<a class="headerlink" href="#convolutional-u-net-architecture" title="Link to this heading"></a></h2>
<p>The U-Net architecture is a type of convolutional neural network (CNN) , The architecture is designed like a U-shape, hence the name U-Net.</p>
</section>
<section id="vision-transformers">
<h2>Vision Transformers<a class="headerlink" href="#vision-transformers" title="Link to this heading"></a></h2>
<p>Vision Transformers (ViT) are a recent development in the field of computer vision that apply transformer models, originally designed for natural language processing tasks, to image classification tasks.</p>
</section>
<section id="understanding-latent-diffusion-models-ldms">
<h2>Understanding Latent Diffusion Models (LDMs)<a class="headerlink" href="#understanding-latent-diffusion-models-ldms" title="Link to this heading"></a></h2>
<p>Latent Diffusion Models (LDMs) are a type of generative model that learn to generate data by modeling it as a diffusion process. This process begins with a simple prior, such as Gaussian noise, and gradually transforms it into the target distribution through a series of small steps</p>
<section id="shifting-towards-transformer-backbone">
<h3>Shifting towards Transformer Backbone<a class="headerlink" href="#shifting-towards-transformer-backbone" title="Link to this heading"></a></h3>
<p>Transformers, originally designed for natural language processing tasks, have shown great potential in computer vision tasks. Unlike convolutional networks, transformers can model long-range dependencies without the need for deep networks or large filters. This is because they use self-attention mechanisms, which allow each element in the input to interact with all other elements, regardless of their distance. Moreover, transformers are not translation invariant, which means they can capture the absolute position of features. This is achieved through the use of positional encodings, which add information about the position of each element in the input.</p>
</section>
<section id="diffusion-transformers-dit">
<h3>Diffusion Transformers (DiT)<a class="headerlink" href="#diffusion-transformers-dit" title="Link to this heading"></a></h3>
<p>DiT uses transformers in a latent diffusion process, where a simple prior (like Gaussian noise) is gradually transformed into the target image. This is done by reversing the diffusion process guided by a transformer network. An important aspect of DiT is the concept of diffusion timesteps. These timesteps represent the stages of the diffusion process, and the transformer network is conditioned on the timestep at each stage. This allows the network to generate different features at different stages of the diffusion process</p>
</section>
<section id="vision-transformers-vit">
<h3>Vision Transformers (ViT)<a class="headerlink" href="#vision-transformers-vit" title="Link to this heading"></a></h3>
<p>ViT uses transformers to directly generate the image in an autoregressive manner, where each patch is generated one after the other, conditioned on the previously generated patches. A key component of ViT is the use of adaptive layer norm layers (adaLN).</p>
</section>
<section id="scalability-of-dit">
<h3>Scalability of DiT<a class="headerlink" href="#scalability-of-dit" title="Link to this heading"></a></h3>
<p>Scalability is an important feature of Diffusion models with Transformers (DiT). As the size of the input data increases, the model should be able to maintain or improve its performance.</p>
</section>
</section>
<section id="dit-scaling-methods">
<h2>DiT Scaling Methods<a class="headerlink" href="#dit-scaling-methods" title="Link to this heading"></a></h2>
<p>There are two primary methods for scaling DiT models: scaling the model size and scaling the number of tokens.</p>
<section id="scaling-model-size">
<h3>Scaling Model Size<a class="headerlink" href="#scaling-model-size" title="Link to this heading"></a></h3>
<p>Scaling the model size involves increasing the complexity of the model, typically by adding more layers or increasing the number of neurons in each layer.</p>
</section>
<section id="scaling-tokens">
<h3>Scaling Tokens<a class="headerlink" href="#scaling-tokens" title="Link to this heading"></a></h3>
<p>Scaling the number of tokens involves increasing the size of the input data that the model can handle.</p>
</section>
</section>
<section id="diffusion-transformers-generalized-architecture">
<h2>Diffusion Transformers Generalized Architecture<a class="headerlink" href="#diffusion-transformers-generalized-architecture" title="Link to this heading"></a></h2>
<section id="spatial-representations">
<h3>Spatial Representations<a class="headerlink" href="#spatial-representations" title="Link to this heading"></a></h3>
<p>The model first inputs spatial representations through a network layer, converting spatial inputs into a sequence of tokens. This process allows the model to handle the spatial information present in the image data.</p>
</section>
<section id="positional-embeddings">
<h3>Positional Embeddings<a class="headerlink" href="#positional-embeddings" title="Link to this heading"></a></h3>
<p>Positional embeddings are a critical component of the transformer architecture. They provide the model with information about the position of each token in the sequence. This process helps the model understand the relative positions and relationships between different parts of the image.</p>
</section>
<section id="dit-block-design">
<h3>DiT Block Design<a class="headerlink" href="#dit-block-design" title="Link to this heading"></a></h3>
<p><img alt="DiTarchit" src="../../_images/dit-arcthecture.avif" /></p>
</section>
</section>
<section id="stable-diffusion-3">
<h2>Stable Diffusion 3<a class="headerlink" href="#stable-diffusion-3" title="Link to this heading"></a></h2>
<p>Stable Diffusion 3 (SD3) is an advanced text-to-image generation model developed by Stability AI. SD3 combines a diffusion transformer architecture and flow matching.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ggangliu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>