

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>How to add a new backend for pytorch &mdash; ggangliu-doc v0.01 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/_variables.scss?v=a5c9e1b9" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0f1f8bc0" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f6c7d6a8"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link href="../../_static/custom.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            ggangliu-doc
              <img src="../../_static/firstai-2.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../ai-deep-learning.html">AI (Deep Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-embedded.html">AI Embedded</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simulator.html">Simulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computer-architecture.html">Computer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hdl.html">Hardware Description Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../open-source-project.html">Open Source Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../work-related.html">Work-related</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ggangliu-doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">How to add a new backend for pytorch</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="how-to-add-a-new-backend-for-pytorch">
<h1>How to add a new backend for pytorch<a class="headerlink" href="#how-to-add-a-new-backend-for-pytorch" title="Link to this heading"></a></h1>
<p>first-riscv-pytorh = FRP
icube</p>
<ul class="simple">
<li><p><a class="reference internal" href="#how-to-add-a-new-backend-for-pytorch"><span class="xref myst">How to add a new backend for pytorch</span></a></p>
<ul>
<li><p><a class="reference internal" href="#%E4%B8%BA%E6%96%B0%E5%90%8E%E7%AB%AF%E6%B3%A8%E5%86%8C%E5%86%85%E6%A0%B8"><span class="xref myst">为新后端注册内核</span></a></p></li>
<li><p><a class="reference internal" href="#%E4%B8%BA%E6%96%B0%E5%90%8E%E7%AB%AF%E6%B3%A8%E5%86%8C%E7%94%9F%E6%88%90%E5%99%A8"><span class="xref myst">为新后端注册生成器</span></a></p></li>
<li><p><a class="reference internal" href="#%E4%B8%BA%E6%96%B0%E5%90%8E%E7%AB%AF%E6%B3%A8%E5%86%8C%E8%AE%BE%E5%A4%87%E9%98%B2%E6%8A%A4"><span class="xref myst">为新后端注册设备防护</span></a></p></li>
<li><p><a class="reference internal" href="#%E4%B8%BA%E6%96%B0%E5%90%8E%E7%AB%AF%E5%85%83%E6%95%B0%E6%8D%AE%E6%B3%A8%E5%86%8C%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E5%87%BD%E6%95%B0"><span class="xref myst">为新后端元数据注册序列化和反序列化函数</span></a></p></li>
<li><p><a class="reference internal" href="#%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9D%97"><span class="xref myst">其他模块</span></a></p></li>
<li><p><a class="reference internal" href="#%E6%8F%90%E5%8D%87%E7%94%A8%E6%88%B7%E7%9A%84%E6%98%93%E7%94%A8%E6%80%A7"><span class="xref myst">提升用户的易用性</span></a></p>
<ul>
<li><p><a class="reference internal" href="#%E5%90%91pytorch%E6%B3%A8%E5%86%8C%E6%96%B0%E7%9A%84%E5%90%8E%E7%AB%AF%E6%A8%A1%E5%9D%97"><span class="xref myst">向PyTorch注册新的后端模块</span></a></p></li>
<li><p><a class="reference internal" href="#%E5%B0%86-privateuse1-%E9%87%8D%E5%91%BD%E5%90%8D%E4%B8%BA%E6%96%B0%E5%90%8E%E7%AB%AF%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8D%E7%A7%B0"><span class="xref myst">将 PrivateUse1 重命名为新后端的自定义名称</span></a></p></li>
<li><p><a class="reference internal" href="#%E7%94%9F%E6%88%90%E4%B8%8E%E6%96%B0%E5%90%8E%E7%AB%AF%E7%9B%B8%E5%85%B3%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E5%B1%9E%E6%80%A7"><span class="xref myst">生成与新后端相关的方法和属性</span></a></p></li>
<li><p><a class="reference internal" href="#%E6%9E%84%E5%BB%BA%E6%89%A9%E5%B1%95"><span class="xref myst">构建扩展</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#defining-schema-and-backend-implementations"><span class="xref myst">Defining schema and backend implementations</span></a></p></li>
<li><p><a class="reference internal" href="#reference"><span class="xref myst">Reference</span></a></p></li>
</ul>
</li>
</ul>
<section id="id1">
<h2>为新后端注册内核<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>将新后端支持的所有前向算子注册到调度程序，同时注册回退 ，这样当新后端不支持某些算子时，这些算子可以回落到CPU执行确保功能的可用性</p></li>
</ol>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">wrapper_Custom_Tensor_add</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Implementation of add kernel in new backend</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">aten</span><span class="p">,</span><span class="w"> </span><span class="n">PrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;add.Tensor&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_FN</span><span class="p">(</span><span class="n">wrapper_Custom_Tensor_add</span><span class="p">));</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="n">custom_cpu_fallback</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">OperatorHandle</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">jit</span><span class="o">::</span><span class="n">Stack</span><span class="o">*</span><span class="w"> </span><span class="n">stack</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Add some hints about new devices that do not support and need to fall back to cpu</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">native</span><span class="o">::</span><span class="n">cpu_fallback</span><span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="w"> </span><span class="n">stack</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="w"> </span><span class="n">PrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">fallback</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">CppFunction</span><span class="o">::</span><span class="n">makeFromBoxedFunction</span><span class="o">&lt;&amp;</span><span class="n">custom_cpu_fallback</span><span class="o">&gt;</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>如果新后端需要覆盖PyTorch Autograd Layer，则通过 AutogradPrivateUse1 将 torch::autograd::Function 的内核注册到调度程序，调度程序和自动求导系统将自动调用这些算子的前向和后向实现</p></li>
</ol>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyAddFunction</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">autograd</span><span class="o">::</span><span class="n">Function</span><span class="o">&lt;</span><span class="n">MyAddFunction</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w"> </span><span class="k">static</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">AutogradContext</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">AutoNonVariableTypeMode</span><span class="w"> </span><span class="n">g</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">myadd</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="w"> </span><span class="p">}</span>

<span class="w"> </span><span class="k">static</span><span class="w"> </span><span class="n">tensor_list</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="n">AutogradContext</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_list</span><span class="w"> </span><span class="n">grad_outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">grad_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">grad_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">grad_output</span><span class="p">,</span><span class="w"> </span><span class="n">grad_output</span><span class="p">};</span>
<span class="w"> </span><span class="p">}</span>
<span class="p">};</span>

<span class="n">Tensor</span><span class="w"> </span><span class="nf">myadd_autograd</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">MyAddFunction</span><span class="o">::</span><span class="n">apply</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">other</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">}</span>

<span class="c1">// Register the autograd kernel to AutogradPrivateUse1</span>
<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">aten</span><span class="p">,</span><span class="w"> </span><span class="n">AutogradPrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="o">&lt;</span><span class="n">myadd_schema</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">myadd_autograd</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// Register the inference kernel to PrivateUse1</span>
<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">aten</span><span class="p">,</span><span class="w"> </span><span class="n">PrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="o">&lt;</span><span class="n">myadd_schema</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">myadd</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>通过 AutocastPrivateUse1 将想要支持自动混合精度（AMP）和回退机制的内核注册到调度程序，当需要时，自动转换系统将自动调用这些内核</p></li>
</ol>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">aten</span><span class="p">,</span><span class="w"> </span><span class="n">AutocastPrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="w">    </span><span class="n">KERNEL_PRIVATEUSEONE</span><span class="p">(</span><span class="o">&lt;</span><span class="k">operator</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="o">&lt;</span><span class="n">policy</span><span class="o">&gt;</span><span class="p">)</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="w"> </span><span class="n">AutocastPrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">fallback</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">CppFunction</span><span class="o">::</span><span class="n">makeFallthrough</span><span class="p">());</span>
<span class="p">}</span>
</pre></div>
</div>
<p>需要补充的是，如果要在新后端支持 AMP，需要通过torch._register_device_module(“backend_name”, BackendModule)注册一个新的BackendModule，并且BackendModule需要具有以下 API：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get_amp_supported_dtype()</span> <span class="pre">-&gt;</span> <span class="pre">List[torch.dtype]</span></code> 在 AMP 中获取新后端支持的dtype，可能支持一个以上的dtype</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">is_autocast_enabled()</span> <span class="pre">-&gt;</span> <span class="pre">bool</span></code> 检查新后端是否启用 AMP</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_autocast_dtype()</span> <span class="pre">-&gt;</span> <span class="pre">torch.dtype</span></code> 在 AMP 中获取新后端支持的dtype，该dtype由set_autocast_dtype或默认dtype设置，而默认dtype为torch.float16</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">set_autocast_enabled(bool)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code> 在新后端上启用或禁用 AMP</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">set_autocast_dtype(dtype)</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code> 在 AMP 中设置新后端支持的dtype，并且dtype包含在从get_amp_supported_dtype获取的dtypes中</p></li>
</ul>
</section>
<section id="id2">
<h2>为新后端注册生成器<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<p>需要支持与新设备对应的生成器</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">CustomGeneratorImpl</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">GeneratorImpl</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Implementation of generator in new backend</span>
<span class="p">}</span>

<span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="w"> </span><span class="n">make_custom_generator</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceIndex</span><span class="w"> </span><span class="n">device_index</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">make_generator</span><span class="o">&lt;</span><span class="n">CustomGeneratorImpl</span><span class="o">&gt;</span><span class="p">(</span><span class="n">device_index</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">REGISTER_GENERATOR_PRIVATEUSE1</span><span class="p">(</span><span class="n">make_cumstom_generator</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id3">
<h2>为新后端注册设备防护<a class="headerlink" href="#id3" title="Link to this heading"></a></h2>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">CustomGuardImpl</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">DeviceGuardImplInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Implementation of guard in new backend</span>
<span class="p">}</span>

<span class="n">C10_REGISTER_GUARD_IMPL</span><span class="p">(</span><span class="n">PrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="n">CustomGuardImpl</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="id4">
<h2>为新后端元数据注册序列化和反序列化函数<a class="headerlink" href="#id4" title="Link to this heading"></a></h2>
<p>PyTorch 目前能够动态注册序列化/反序列化函数，以支持在TensorImpl.ExtraMeta类中命名为backend_meta_的新后端附加元数据的序列化和反序列化。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">CustomBackendMetadata</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">BackendMeta</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Implementation of backend metadata in new backend</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="n">for_serialization</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Implementation of serialization</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="n">for_deserialization</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Implementation of deserialization</span>
<span class="p">}</span>

<span class="n">TensorBackendMetaRegistry</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">PrivateUse1</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">for_serialization</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">for_deserialization</span><span class="p">);</span><span class="w">    </span>
</pre></div>
</div>
</section>
<section id="id5">
<h2>其他模块<a class="headerlink" href="#id5" title="Link to this heading"></a></h2>
</section>
<section id="id6">
<h2>提升用户的易用性<a class="headerlink" href="#id6" title="Link to this heading"></a></h2>
<section id="pytorch">
<h3>向PyTorch注册新的后端模块<a class="headerlink" href="#pytorch" title="Link to this heading"></a></h3>
<p>PyTorch 中一些 CUDA 相关的接口可以通过以下形式调用： <code class="docutils literal notranslate"><span class="pre">torch.cuda.xxx</span></code> 。因此，为了符合用户习惯，通过PrivateUse1机制实现的新后端也应该提供类似的接口。例如，Ascend NPU:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="p">.</span><span class="n">_register_device_module</span><span class="p">(</span><span class="err">&#39;</span><span class="n">npu</span><span class="err">&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">torch_npu</span><span class="p">.</span><span class="n">npu</span><span class="p">)</span>
</pre></div>
</div>
<p>完成上述操作后，用户可以通过 <code class="docutils literal notranslate"><span class="pre">torch.npu.xxx</span></code></p>
</section>
<section id="privateuse1">
<h3>将 PrivateUse1 重命名为新后端的自定义名称<a class="headerlink" href="#privateuse1" title="Link to this heading"></a></h3>
<p>PrivateUse1 Key 是集成到 PyTorch 中的新后端的内部机制。对于用户来说，与 PrivateUse1 相比，使用新后端强相关的自定义名称应该更加友好。</p>
<p>以 <code class="docutils literal notranslate"><span class="pre">Ascend</span> <span class="pre">NPU</span></code> 为例，第一个使用会更方便。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="err">&#39;</span><span class="n">npu</span><span class="o">:</span><span class="mi">0</span><span class="err">&#39;</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="err">&#39;</span><span class="n">privateuse1</span><span class="o">:</span><span class="mi">0</span><span class="err">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch 为自命名的 PrivateUse1 后端提供了一个新的 C++/Python API，使用起来非常简单。</p>
<p>Python</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">rename_privateuse1_backend</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>C++</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">c10</span><span class="o">::</span><span class="n">register_privateuse1_backend</span><span class="p">(</span><span class="s">&quot;npu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id7">
<h3>生成与新后端相关的方法和属性<a class="headerlink" href="#id7" title="Link to this heading"></a></h3>
<p>将 PrivateUse1 重命名为自定义名称后，在新后端的 Tensor, nn, Storage 模块中自动生成与新后端名称相关的属性和方法。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">rename_privateuse1_backend</span><span class="p">(</span><span class="s2">&quot;npu&quot;</span><span class="p">)</span>
<span class="n">unsupported_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">]</span>
<span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">generate_methods_for_privateuse1_backend</span><span class="p">(</span><span class="n">for_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">for_module</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">for_storage</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unsupported_dtype</span><span class="o">=</span><span class="n">unsupported_dtype</span><span class="p">)</span> 

<span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">is_npu</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Storage</span><span class="o">.</span><span class="n">npu</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Storage</span><span class="o">.</span><span class="n">is_npu</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>构建扩展<a class="headerlink" href="#id8" title="Link to this heading"></a></h3>
<p>通过向 PyTorch 添加 C++ 扩展来支持树外后端。 一旦准备好内核和注册，您就可以通过编写一个 setup.py 脚本来构建 C++ 扩展，该脚本使用 setuptools 编译 C++ 代码。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="kn">import</span> <span class="n">setup</span>
<span class="kn">from</span> <span class="nn">torch.utils.cpp_extension</span> <span class="kn">import</span> <span class="n">BuildExtension</span><span class="p">,</span> <span class="n">CppExtension</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;torch_xla&#39;</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span> <span class="n">VERSION</span><span class="p">,</span>
    <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="n">CppExtension</span><span class="p">(</span>
            <span class="s1">&#39;_XLAC&#39;</span><span class="p">,</span>
            <span class="n">torch_xla_sources</span><span class="p">,</span>
            <span class="n">include_dirs</span><span class="o">=</span><span class="n">include_dirs</span><span class="p">,</span>
            <span class="n">extra_compile_args</span><span class="o">=</span><span class="n">extra_compile_args</span><span class="p">,</span>
            <span class="n">library_dirs</span><span class="o">=</span><span class="n">library_dirs</span><span class="p">,</span>
            <span class="n">extra_link_args</span><span class="o">=</span><span class="n">extra_link_args</span> <span class="o">+</span> \
                <span class="p">[</span><span class="n">make_relative_rpath</span><span class="p">(</span><span class="s1">&#39;torch_xla/lib&#39;</span><span class="p">)],</span>
        <span class="p">),</span>
    <span class="p">],</span>
    <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;build_ext&#39;</span><span class="p">:</span> <span class="n">Build</span><span class="p">,</span>  <span class="c1"># Build is a derived class of BuildExtension</span>
    <span class="p">}</span>
    <span class="c1"># more configs...</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="defining-schema-and-backend-implementations">
<h2>Defining schema and backend implementations<a class="headerlink" href="#defining-schema-and-backend-implementations" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">TORCH_LIBRARY</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="o">.</span><span class="n">def</span><span class="p">(</span><span class="s2">&quot;myadd(Tensor self, Tensor other) -&gt; Tensor&quot;</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">Tensor</span> <span class="n">myadd_cpu</span><span class="p">(</span><span class="n">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">self_</span><span class="p">,</span> <span class="n">const</span> <span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">other_</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">self_</span><span class="o">.</span><span class="n">sizes</span><span class="p">()</span> <span class="o">==</span> <span class="n">other_</span><span class="o">.</span><span class="n">sizes</span><span class="p">());</span>
  <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">self_</span><span class="o">.</span><span class="n">device</span><span class="p">()</span><span class="o">.</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="p">::</span><span class="n">CPU</span><span class="p">);</span>
  <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">other_</span><span class="o">.</span><span class="n">device</span><span class="p">()</span><span class="o">.</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">DeviceType</span><span class="p">::</span><span class="n">CPU</span><span class="p">);</span>
  <span class="n">Tensor</span> <span class="bp">self</span> <span class="o">=</span> <span class="n">self_</span><span class="o">.</span><span class="n">contiguous</span><span class="p">();</span>
  <span class="n">Tensor</span> <span class="n">other</span> <span class="o">=</span> <span class="n">other_</span><span class="o">.</span><span class="n">contiguous</span><span class="p">();</span>
  <span class="n">Tensor</span> <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">::</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sizes</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">());</span>
  <span class="n">const</span> <span class="nb">float</span><span class="o">*</span> <span class="n">self_ptr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="n">const</span> <span class="nb">float</span><span class="o">*</span> <span class="n">other_ptr</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="nb">float</span><span class="o">*</span> <span class="n">result_ptr</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="nb">float</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="n">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">result</span><span class="o">.</span><span class="n">numel</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">result_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">other_ptr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">myops</span><span class="p">,</span> <span class="n">CPU</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">m</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="s2">&quot;myadd&quot;</span><span class="p">,</span> <span class="n">myadd_cpu</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/advanced/extend_dispatcher.html">https://pytorch.org/tutorials/advanced/extend_dispatcher.html</a></p></li>
<li><p><a class="reference external" href="https://github.com/ascend/pytorch">https://github.com/ascend/pytorch</a></p></li>
<li><p><a class="reference external" href="https://www.cnblogs.com/apachecn/p/18006835">https://www.cnblogs.com/apachecn/p/18006835</a></p></li>
<li><p><a class="reference external" href="http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/">http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html</a></p></li>
<li><p><a class="reference external" href="https://github.com/sandeepkumar-skb/pytorch_custom_op">https://github.com/sandeepkumar-skb/pytorch_custom_op</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ggangliu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>