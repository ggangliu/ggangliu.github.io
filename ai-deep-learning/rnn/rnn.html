

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Recurrent Neural Networks &mdash; ggangliu-doc v0.01 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/_variables.scss?v=a5c9e1b9" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0f1f8bc0" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f6c7d6a8"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Diffusion" href="../diffusion/diffusion.html" />
    <link rel="prev" title="Learning Pytorch with Example" href="../learning_pytorch.html" />
    <link href="../../_static/custom.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            ggangliu-doc
              <img src="../../_static/firstai-2.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../ai-deep-learning.html">AI (Deep Learning)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../deep-learning-course.html">How to study Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../learning_pytorch.html">Learning Pytorch with Example</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Recurrent Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-problem-of-long-term-dependencies">The problem of long-term dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#long-short-term-memory-lstm">Long-Short Term Memory (LSTM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-problem-with-lstms">The problem with LSTMs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#attention">Attention</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convolutional-neural-networks">Convolutional Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transformer">Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#self-attention">Self-Attention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#self-attention-layer">Self-Attention Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multihead-attention">Multihead attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="#positional-encoding">Positional Encoding</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../diffusion/diffusion.html">Diffusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sd3/stable-diffusion-3-medium.html">Stable Diffusion 3 Medium</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative-ai-for-beginners.html">Generative AI for Beginners (Version 2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ai-for-beginners.html">Artificial Intelligence for Beginners</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformer/transformer.html">Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vllm/vllm.html">vLLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lightllm/lightllm.html">LightLLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tokenattention/tokenattention.html">Token Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../flashattention/flashattention.html">Flash Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../faster-transformer/faster-transformer.html">FasterTransformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kimi/mooncake.html">Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-embedded.html">AI Embedded</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../simulator.html">Simulator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../compiler.html">Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computer-architecture.html">Computer Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hdl.html">Hardware Description Language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../open-source-project.html">Open Source Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../project.html">Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../work-related.html">Work-related</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ggangliu-doc</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../ai-deep-learning.html">AI (Deep Learning)</a></li>
      <li class="breadcrumb-item active">Recurrent Neural Networks</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="recurrent-neural-networks">
<h1>Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Link to this heading"></a></h1>
<p><img alt="rnn" src="../../_images/rnn.webp" /></p>
<p>The following picture shows how usually a sequence to sequence model works using Recurrent Neural Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.</p>
<p><img alt="rnn-show" src="../../_images/rnn.gif" /></p>
<section id="the-problem-of-long-term-dependencies">
<h2>The problem of long-term dependencies<a class="headerlink" href="#the-problem-of-long-term-dependencies" title="Link to this heading"></a></h2>
<p><img alt="long-term" src="../../_images/rnn1.webp" /></p>
<p>RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.</p>
<p>In theory, RNNs could learn this long-term dependencies. In practice, they don’t seem to learn them. LSTM, a special type of RNN, tries to solve this kind of problem.</p>
</section>
<section id="long-short-term-memory-lstm">
<h2>Long-Short Term Memory (LSTM)<a class="headerlink" href="#long-short-term-memory-lstm" title="Link to this heading"></a></h2>
<p>LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. In this way, LSTMs can selectively remember or forget things that are important and not so important.</p>
<p><img alt="lstm" src="../../_images/lstm.webp" /></p>
<p>Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state and the output of the previous cell. It manipulates these inputs and based on them, it generates a new cell state, and an output.</p>
</section>
<section id="the-problem-with-lstms">
<h2>The problem with LSTMs<a class="headerlink" href="#the-problem-with-lstms" title="Link to this heading"></a></h2>
<p>The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are too long LSTMs still don’t do too well. he reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.</p>
<p>That means that when sentences are long, the model often forgets the content of distant positions in the sequence. Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you are have to process word by word. To summarize, LSTMs and RNNs present 3 problems:</p>
<ul class="simple">
<li><p>Sequential computation inhibits parallelization</p></li>
<li><p>No explicit modeling of long and short range dependencies</p></li>
<li><p>“Distance” between positions is linear</p></li>
</ul>
</section>
<section id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Link to this heading"></a></h2>
<p>To solve these problems, Attention is a technique that is used in a neural network.</p>
<p><img alt="rnn-attention" src="../../_images/rnn-attention.gif" /></p>
<p>For attention to be brought to RNNs in sequence transduction, we divide the encoding and decoding into 2 main steps. One step is represented in green and the other in purple. The green step is called the encoding stage and the purple step is the decoding stage.</p>
<p><img alt="rnn-at" src="../../_images/rnn-attention1.gif" /></p>
<p><img alt="rnn-case" src="../../_images/rnn-attention2.gif" /></p>
<p>But some of the problems that we discussed, still are not solved with RNNs using attention. For example, processing inputs (words) in parallel is not possible. For a large corpus of text, this increases the time spent translating the text.</p>
</section>
<section id="convolutional-neural-networks">
<h2>Convolutional Neural Networks<a class="headerlink" href="#convolutional-neural-networks" title="Link to this heading"></a></h2>
<p>Convolutional Neural Networks help solve these problems.</p>
<p><img alt="cnn" src="../../_images/cnn.gif" /></p>
<p>The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated.</p>
<p>The problem is that Convolutional Neural Networks do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That’s why Transformers were created, they are a combination of both CNNs with attention.</p>
</section>
<section id="transformer">
<h2>Transformer<a class="headerlink" href="#transformer" title="Link to this heading"></a></h2>
<p>To solve the problem of parallelization, Transformers try to solve the problem by using encoders and decoders together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another.</p>
<p><img alt="transformer" src="../../_images/transformer.webp" /></p>
<p>Internally, the Transformer has a similar kind of architecture as the previous models above. But the Transformer consists of six encoders and six decoders.</p>
<p><img alt="transformer1" src="../../_images/transformer1.webp" /></p>
<p>Each encoder consists of two layers: <strong>Self-attention</strong> and <strong>a feed Forward Neural Network</strong>.</p>
<p><img alt="fc" src="../../_images/encoder.webp" /></p>
<p>The encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.</p>
<p><img alt="decoder" src="../../_images/decoder.webp" /></p>
</section>
<section id="self-attention">
<h2>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading"></a></h2>
<p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm. Each word is embedded into a vector of size 512.</p>
<p><img alt="embedding" src="../../_images/embedding.webp" /></p>
<p><img alt="encoder2" src="../../_images/encoder2.webp" /></p>
<section id="self-attention-layer">
<h3>Self-Attention Layer<a class="headerlink" href="#self-attention-layer" title="Link to this heading"></a></h3>
<p>Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented — using matrices.</p>
<p><img alt="self-attention" src="../../_images/self-attention.webp" /></p>
<p>The <strong>first step</strong> in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>
<p><img alt="embedded-input" src="../../_images/embedded-input.webp" /></p>
<p>Multiplying x1 by the WQ weight matrix produces q1, the “query” vector associated with that word. We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence.</p>
<p>What are the “query”, “key”, and “value” vectors?</p>
<p>The <strong>second step</strong> in calculating self-attention is to calculate a score. The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring.</p>
<p><img alt="step2" src="../../_images/step2.webp" /></p>
<p>The <strong>third and forth steps</strong> are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper — 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p>
<p><img alt="step34" src="../../_images/step34.webp" /></p>
<p>The <strong>fifth step</strong> is to multiply each value vector by the softmax score (in preparation to sum them up).</p>
<p>The <strong>sixth step</strong> is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p>
</section>
<section id="multihead-attention">
<h3>Multihead attention<a class="headerlink" href="#multihead-attention" title="Link to this heading"></a></h3>
<p>The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking.</p>
<p><img alt="multihead" src="../../_images/multi-head1.webp" />
<img alt="multihead" src="../../_images/multi-head2.webp" />
<img alt="multihead" src="../../_images/multi-head3.webp" /></p>
</section>
<section id="positional-encoding">
<h3>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading"></a></h3>
<p>Another important step on the Transformer is to add positional encoding when encoding each word. Encoding the position of each word is relevant, since the position of each word is relevant to the translation.</p>
</section>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/transformers-141e32e69591">https://towardsdatascience.com/transformers-141e32e69591</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../learning_pytorch.html" class="btn btn-neutral float-left" title="Learning Pytorch with Example" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../diffusion/diffusion.html" class="btn btn-neutral float-right" title="Diffusion" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, ggangliu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>